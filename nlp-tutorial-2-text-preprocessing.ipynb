{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* **What really is Text Preprocessing?**\n\n**Basically Text preprocessing in Natural Language Processing (NLP) involves a set of tasks and techniques aimed at cleaning and transforming raw text data into a format that is suitable for analysis and machine learning models. It plays a crucial role in enhancing the quality and effectiveness of NLP applications by addressing various challenges associated with unstructured text.**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-23T10:56:01.785831Z","iopub.execute_input":"2023-12-23T10:56:01.786328Z","iopub.status.idle":"2023-12-23T10:56:01.792161Z","shell.execute_reply.started":"2023-12-23T10:56:01.786285Z","shell.execute_reply":"2023-12-23T10:56:01.791232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:01.793860Z","iopub.execute_input":"2023-12-23T10:56:01.795619Z","iopub.status.idle":"2023-12-23T10:56:02.586524Z","shell.execute_reply.started":"2023-12-23T10:56:01.795573Z","shell.execute_reply":"2023-12-23T10:56:02.585225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**N.B. Please be aware that I will apply several procedures here, and these may not be directly relevant to the current dataset. Nevertheless, I will implement them for demonstration purposes. If your dataset necessitates these procedures for preprocessing, you can readily adapt and apply them to your specific dataset.**","metadata":{}},{"cell_type":"markdown","source":"# Step 01: Lowercasing","metadata":{}},{"cell_type":"markdown","source":"Let's consider two sentences \"Blue sky and puffy clouds are so pleasent to watch\" and \"The sky is blue.\" If we notice here, the word 'BLUE' in both sentences referring to the blue color.The difference is one started with capital B and another one with small b. But the algorithm will consider them as two different words('Blue' and 'blue') even though these are same words. So in order to avoid this kind of unnecessary complexities, the first thing we do is we lowercase the sentences.\n\n\nLowercasing is a crucial step in text preprocessing for Natural Language Processing (NLP). It involves converting all letters in a text to lowercase, serving several purposes.It ensures uniformity, treating words with different cases as identical and simplifying subsequent analyses. Lowercasing also contributes to word normalization, reducing words to a consistent case, such as transforming \"Blue\" and \"blue\" to the same representation.","metadata":{}},{"cell_type":"code","source":"df['review'] = df['review'].str.lower()\n\n# After lowercasing all the reviews\ndf['review'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:02.587772Z","iopub.execute_input":"2023-12-23T10:56:02.588110Z","iopub.status.idle":"2023-12-23T10:56:02.810482Z","shell.execute_reply.started":"2023-12-23T10:56:02.588081Z","shell.execute_reply":"2023-12-23T10:56:02.808952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 02: Removing HTML tags","metadata":{}},{"cell_type":"markdown","source":"Eliminating HTML tags is a crucial preprocessing step in NLP because HTML tags, primarily used for web page formatting, often introduce noise and structural information that is irrelevant to language analysis tasks. The focus in NLP is typically on extracting meaningful content from the text rather than the webpage structure. By removing HTML tags, we ensure consistency in text representation, prevent confusion for NLP models, facilitate improved tokenization, and enhance overall readability. ","metadata":{}},{"cell_type":"markdown","source":"**N.B. Please note that moving forward, we will be employing Regular Expressions (RegEx). Therefore, it is essential to have a moderate understanding of Regular Expressions before delving into the realm of text preprocessing in Natural Language Processing (NLP).**","metadata":{}},{"cell_type":"code","source":"import re \ndef remove_html_tags(text):\n    pattern = re.compile('<.*?>')\n    return pattern.sub(r'',text)\ndf['review'] = df['review'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:02.813266Z","iopub.execute_input":"2023-12-23T10:56:02.813683Z","iopub.status.idle":"2023-12-23T10:56:03.078797Z","shell.execute_reply.started":"2023-12-23T10:56:02.813650Z","shell.execute_reply":"2023-12-23T10:56:03.077493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After removing all the html tags\ndf['review'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.080251Z","iopub.execute_input":"2023-12-23T10:56:03.080618Z","iopub.status.idle":"2023-12-23T10:56:03.089426Z","shell.execute_reply.started":"2023-12-23T10:56:03.080588Z","shell.execute_reply":"2023-12-23T10:56:03.088444Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 03: Removing URL","metadata":{}},{"cell_type":"markdown","source":"\nRemoving URLs from text data is a common preprocessing step in NLP for several reasons. URLs, or web links, often carry information related to web formatting and navigation that is irrelevant or even detrimental to certain NLP tasks. By eliminating URLs, the focus shifts to the textual content, allowing NLP models to extract meaningful patterns without interference from web-specific elements. URLs may introduce noise, complicate tokenization, and affect the overall consistency of the text representation. ","metadata":{}},{"cell_type":"markdown","source":"**Although there is no URL in this particular dataset but this is  for demonstration purpose. So if you encounter with URLs in your dataset, we can follow this procedure.**","metadata":{}},{"cell_type":"code","source":"def remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www.\\.\\S+')\n    return pattern.sub(r'',text)\n\n# Let's consider a text \ntext1 = \"The official website of Google is https://www.google.com/\"\n# Now we are applying the function we have made earlier in this text\nremove_urls = remove_url(text1)\nprint(remove_urls)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.090577Z","iopub.execute_input":"2023-12-23T10:56:03.091115Z","iopub.status.idle":"2023-12-23T10:56:03.099563Z","shell.execute_reply.started":"2023-12-23T10:56:03.091081Z","shell.execute_reply":"2023-12-23T10:56:03.098351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 04: Removing Punctuation","metadata":{}},{"cell_type":"markdown","source":"\nPunctuation marks are symbols such as periods, commas, exclamation points,question marks etc that are used in written language to indicate pauses, boundaries, or the tone of a sentence. In NLP, removing punctuation is a common preprocessing step for several reasons. Punctuation marks may introduce noise to the text, and their presence can impact the accuracy and efficiency of various NLP tasks. When analyzing text, NLP models often focus on the semantics of words and sentences rather than the syntactic nuances introduced by punctuation. By eliminating punctuation, the text becomes more uniform, tokenization becomes simpler, and the overall representation of the language becomes cleaner. ","metadata":{}},{"cell_type":"code","source":"import string\npunctuation_list = string.punctuation\nprint(f'The punctuations are {punctuation_list}')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.101068Z","iopub.execute_input":"2023-12-23T10:56:03.101519Z","iopub.status.idle":"2023-12-23T10:56:03.115280Z","shell.execute_reply.started":"2023-12-23T10:56:03.101486Z","shell.execute_reply":"2023-12-23T10:56:03.114080Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_punctuation(text):\n    for char in punctuation_list:\n        text = text.replace(char,'')\n    return text\n\ntext2 = \"In a bustling city, the streets were filled with people rushing to catch trains,buses & taxis!\"\nrem_punct = remove_punctuation(text2)\nprint(rem_punct)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.116821Z","iopub.execute_input":"2023-12-23T10:56:03.117191Z","iopub.status.idle":"2023-12-23T10:56:03.126485Z","shell.execute_reply.started":"2023-12-23T10:56:03.117144Z","shell.execute_reply":"2023-12-23T10:56:03.125240Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 05: Removing Chat Words","metadata":{}},{"cell_type":"markdown","source":"Chat words, also known as chat language or text speak, refer to informal and abbreviated words and expressions commonly used in online communication, particularly in chat rooms, messaging apps, and social media platforms.In NLP, removing chat words is a crucial preprocessing step for several reasons. Chat words can introduce noise and ambiguity into the text data, making it challenging for NLP models to accurately interpret and analyze the content. These informal expressions often deviate from standard grammar and vocabulary, leading to a less consistent and more challenging dataset for language processing tasks. By eliminating chat words, the focus shifts to a more standardized and formal representation of language.","metadata":{}},{"cell_type":"code","source":"chat_words ={\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'B4N': 'Bye For Now',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': 'For What It\\'s Worth',\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you (also a chat program)',\n    'ILU': 'ILU: I Love You',\n    'IMHO': 'In My Honest/Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My A.. Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'PITA': 'Pain In The A..',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The F...',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait...',\n    '7K': 'Sick:-D Laugher',\n    'TFW': 'That feeling when',\n    'MFW': 'My face when',\n    'MRW': 'My reaction when',\n    'IFYP': 'I feel your pain',\n    'LOL': 'Laughing out loud',\n    'TNTL': 'Trying not to laugh',\n    'JK': 'Just kidding',\n    'IDC': 'I don’t care',\n    'ILY': 'I love you',\n    'IMU': 'I miss you',\n    'ADIH': 'Another day in hell',\n    'IDC': 'I don’t care',\n    'ZZZ': 'Sleeping, bored, tired',\n    'WYWH': 'Wish you were here',\n    'TIME': 'Tears in my eyes',\n    'BAE': 'Before anyone else',\n    'FIMH': 'Forever in my heart',\n    'BSAAW': 'Big smile and a wink',\n    'BWL': 'Bursting with laughter',\n    'LMAO': 'Laughing my a** off',\n    'BFF': 'Best friends forever',\n    'CSL': 'Can’t stop laughing'\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.128364Z","iopub.execute_input":"2023-12-23T10:56:03.128763Z","iopub.status.idle":"2023-12-23T10:56:03.143304Z","shell.execute_reply.started":"2023-12-23T10:56:03.128723Z","shell.execute_reply":"2023-12-23T10:56:03.142363Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chat_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words:\n            new_text.append(chat_words[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.147166Z","iopub.execute_input":"2023-12-23T10:56:03.149305Z","iopub.status.idle":"2023-12-23T10:56:03.159626Z","shell.execute_reply.started":"2023-12-23T10:56:03.149246Z","shell.execute_reply":"2023-12-23T10:56:03.158528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = chat_conversion(\"LOL this is very funny\")\nb = chat_conversion(\"TTYL , I'm in a hurry\")\nc = chat_conversion(\"Send him the mail ASAP\")\nprint(a)\nprint(b)\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.161008Z","iopub.execute_input":"2023-12-23T10:56:03.161378Z","iopub.status.idle":"2023-12-23T10:56:03.173008Z","shell.execute_reply.started":"2023-12-23T10:56:03.161347Z","shell.execute_reply":"2023-12-23T10:56:03.171847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 06: Spell Checker","metadata":{}},{"cell_type":"markdown","source":"**The libraries used for spell checking are TextBlob,enchanter,spellchecker,pyspellchecker etc.**","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nincorrect_text = \"Suddnly, the weathr became very cold and it startd to snow unexpectedly.\"\ntxtblb = TextBlob(incorrect_text)\ntxtblb.correct().string","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.174667Z","iopub.execute_input":"2023-12-23T10:56:03.175664Z","iopub.status.idle":"2023-12-23T10:56:03.500749Z","shell.execute_reply.started":"2023-12-23T10:56:03.175625Z","shell.execute_reply":"2023-12-23T10:56:03.499450Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 07: Removing Stopwords","metadata":{}},{"cell_type":"markdown","source":"Stopwords in English are commonly used words that are often considered to be of little value in text analysis due to their high frequency and general utility in constructing sentences. \n\nExamples of stop words in English include:\n\n* Common Pronouns: I, me, he, she, it, you, they, we, us, them.\n* Prepositions: in, on, at, by, with, under, over, between, through, etc.\n* Conjunctions: and, or, but, so, for, nor, yet.\n* Articles: a, an, the.\n* Common Verbs: is, am, are, was, were, be, being, been, have, has, had, do, does, did\n\nWe remove stop words before approaching problems like Sentiment analysis but in the case of POS tagging (Parts Of Speech tagging) we don't remove stop words! So we have to be careful in these cases.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words('english') # Stopwords in English\nstopwords.words('french') # Stopwords in French\nstopwords.words('spanish') # Stopwords in Spanish","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-23T10:56:03.502631Z","iopub.execute_input":"2023-12-23T10:56:03.503119Z","iopub.status.idle":"2023-12-23T10:56:03.508852Z","shell.execute_reply.started":"2023-12-23T10:56:03.503074Z","shell.execute_reply":"2023-12-23T10:56:03.507558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.corpus import stopwords\ndef remove_stopwords(text):\n    new_text = []\n    for word in text.split():\n        if word.lower() not in stopwords.words('english'):\n            new_text.append(word)\n    return ' '.join(new_text)\n\n\ntext = \"I was roaming around a jungle while I saw a tiger.It was sleeping under a tree. I waited there for three hours\"\nremove_stopwords(text)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.510356Z","iopub.execute_input":"2023-12-23T10:56:03.510865Z","iopub.status.idle":"2023-12-23T10:56:03.533912Z","shell.execute_reply.started":"2023-12-23T10:56:03.510820Z","shell.execute_reply":"2023-12-23T10:56:03.532674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 08: Remove emojis 😊","metadata":{}},{"cell_type":"code","source":"def remove_emojis(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n                               u\"\\U000024C2-\\U0001F251\" \n                               \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\ntext_with_emojis = \"Hello! 😊 This is a sample text with emojis. 🌟\"\ntext_without_emojis = remove_emojis(text_with_emojis)\n\nprint(\"Original text:\", text_with_emojis)\nprint(\"Text without emojis:\", text_without_emojis)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.535923Z","iopub.execute_input":"2023-12-23T10:56:03.536407Z","iopub.status.idle":"2023-12-23T10:56:03.545712Z","shell.execute_reply.started":"2023-12-23T10:56:03.536362Z","shell.execute_reply":"2023-12-23T10:56:03.544539Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 09: Tokenization","metadata":{}},{"cell_type":"markdown","source":"Tokenization is a fundamental process in NLP that involves breaking down a given text into individual units, referred to as tokens. These tokens can be words, phrases, or other meaningful elements, depending on the granularity of the analysis. The primary goal of tokenization is to transform unstructured text into a format that is suitable for further analysis and processing.\n\nIn easier words, Tokenization is a preprocessing step designed to make raw text understandable and manageable for machines. By breaking down the text into smaller units (tokens), machines can effectively analyze and interpret the language. Each token represents a meaningful unit, such as a word or a phrase, allowing the machine to grasp the structures and patterns within the text.","metadata":{}},{"cell_type":"markdown","source":"**A. Using the Split function**","metadata":{}},{"cell_type":"code","source":"# Word tokenization\nsentence1 = \"I am going to Australia\"\nword_tokens = sentence1.split()\nprint(word_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.547912Z","iopub.execute_input":"2023-12-23T10:56:03.548454Z","iopub.status.idle":"2023-12-23T10:56:03.556842Z","shell.execute_reply.started":"2023-12-23T10:56:03.548387Z","shell.execute_reply":"2023-12-23T10:56:03.555658Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sentence tokenization\nsentence2 = \"I am going to Australia.I will stay there for 5 days.I hope the tour will be great\"\nsentence_tokens = sentence2.split('.')\nprint(sentence_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.559131Z","iopub.execute_input":"2023-12-23T10:56:03.559644Z","iopub.status.idle":"2023-12-23T10:56:03.570232Z","shell.execute_reply.started":"2023-12-23T10:56:03.559601Z","shell.execute_reply":"2023-12-23T10:56:03.569270Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Problems in using splilt function (in word tokenization)\nsentence3 = \"I am going to Australia!\"\nsentence3.split()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.571678Z","iopub.execute_input":"2023-12-23T10:56:03.572018Z","iopub.status.idle":"2023-12-23T10:56:03.588494Z","shell.execute_reply.started":"2023-12-23T10:56:03.571988Z","shell.execute_reply":"2023-12-23T10:56:03.587491Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**In the provided code, the use of the split() function resulted in tokenizing \"Australia\" as \"Australia!\" due to the presence of an exclamation mark. Consequently, the machine would treat \"Australia!\" and \"Australia\" as distinct words. This creates an issue because, in subsequent instances, if the machine encounters the word \"Australia\" without an exclamation mark, it may not recognize it as the same entity. So using split() function becomes problematic in such cases.**","metadata":{}},{"cell_type":"code","source":"# Problems in using splilt function (in sentence tokenization)\n\nsentence4 = \"Where do you live? I haven't seen you here before\"\nsentence4.split('.')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.590295Z","iopub.execute_input":"2023-12-23T10:56:03.590676Z","iopub.status.idle":"2023-12-23T10:56:03.602728Z","shell.execute_reply.started":"2023-12-23T10:56:03.590643Z","shell.execute_reply":"2023-12-23T10:56:03.601417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**In sentence tokenization, we can see that two different sentence got tokenized in one as we splitted on the basis of   '.'**","metadata":{}},{"cell_type":"markdown","source":"**B. Using Regular Expression**","metadata":{}},{"cell_type":"code","source":"sentence3 = \"I am going to Australia!\"\ntokens = re.findall(\"[\\w]+\",sentence3)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.604280Z","iopub.execute_input":"2023-12-23T10:56:03.604669Z","iopub.status.idle":"2023-12-23T10:56:03.614034Z","shell.execute_reply.started":"2023-12-23T10:56:03.604635Z","shell.execute_reply":"2023-12-23T10:56:03.613049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nWhile regular expressions offer a slight performance advantage over the split() function for tokenization, they still have their limitations. A more effective approach for tokenization is to leverage specialized libraries. In this case, we'll make use of the NLTK and Spacy libraries. These libraries are designed for natural language processing tasks and provide robust and efficient tokenization mechanisms.","metadata":{}},{"cell_type":"markdown","source":"**C. Using NLTK (Natural Language ToolKit)**","metadata":{}},{"cell_type":"markdown","source":"NLTK is a powerful Python library designed for working with human language data. NLTK provides access to a vast collection of corpora, lexical resources, and algorithms, making it a valuable resource","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize,word_tokenize\nsentence3 = \"I am going to Australia!\"\nword_tokenize(sentence3)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.615581Z","iopub.execute_input":"2023-12-23T10:56:03.615950Z","iopub.status.idle":"2023-12-23T10:56:03.627744Z","shell.execute_reply.started":"2023-12-23T10:56:03.615918Z","shell.execute_reply":"2023-12-23T10:56:03.626544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence4 = \"Where do you live? I haven't seen you here before\"\nsent_tokenize(sentence4)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.629914Z","iopub.execute_input":"2023-12-23T10:56:03.630391Z","iopub.status.idle":"2023-12-23T10:56:03.643849Z","shell.execute_reply.started":"2023-12-23T10:56:03.630348Z","shell.execute_reply":"2023-12-23T10:56:03.642718Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's experiment the NLTK library with some complex sentences\nsentence5 = \"I have a Ph.D in A.I\"\nsentence6 = \"We're here to help! Mail us at abc@gmail.com\"\nsentence7 = \"A 5km ride cost $10.50\"","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.645666Z","iopub.execute_input":"2023-12-23T10:56:03.646359Z","iopub.status.idle":"2023-12-23T10:56:03.656800Z","shell.execute_reply.started":"2023-12-23T10:56:03.646317Z","shell.execute_reply":"2023-12-23T10:56:03.655466Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_tokenize(sentence5)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.658840Z","iopub.execute_input":"2023-12-23T10:56:03.659261Z","iopub.status.idle":"2023-12-23T10:56:03.672690Z","shell.execute_reply.started":"2023-12-23T10:56:03.659203Z","shell.execute_reply":"2023-12-23T10:56:03.671273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PERFECT!","metadata":{}},{"cell_type":"code","source":"word_tokenize(sentence6)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.674256Z","iopub.execute_input":"2023-12-23T10:56:03.675008Z","iopub.status.idle":"2023-12-23T10:56:03.684295Z","shell.execute_reply.started":"2023-12-23T10:56:03.674972Z","shell.execute_reply":"2023-12-23T10:56:03.683370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_tokenize(sentence7)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.686056Z","iopub.execute_input":"2023-12-23T10:56:03.686464Z","iopub.status.idle":"2023-12-23T10:56:03.700001Z","shell.execute_reply.started":"2023-12-23T10:56:03.686416Z","shell.execute_reply":"2023-12-23T10:56:03.698688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So we can see that NLTK is also not givig perfect results in sentences like 6 and 7. But certainly it performs better than split() and regular expression.\n\nA library called Spacy is great for tokenization! Let's see!","metadata":{}},{"cell_type":"markdown","source":"Spacy is a leading open-source library for advanced NLP tasks in Python. It is designed for efficient and production-ready processing of large volumes of text data. Spacy excels in tasks such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing. Known for its speed and accuracy. ","metadata":{}},{"cell_type":"markdown","source":"**D. Using Spacy**","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:03.701600Z","iopub.execute_input":"2023-12-23T10:56:03.702069Z","iopub.status.idle":"2023-12-23T10:56:04.963801Z","shell.execute_reply.started":"2023-12-23T10:56:03.702025Z","shell.execute_reply":"2023-12-23T10:56:04.962628Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"doc1 = nlp(sentence5)\ndoc2 = nlp(sentence6)\ndoc3 = nlp(sentence7)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:04.969766Z","iopub.execute_input":"2023-12-23T10:56:04.970201Z","iopub.status.idle":"2023-12-23T10:56:05.004102Z","shell.execute_reply.started":"2023-12-23T10:56:04.970153Z","shell.execute_reply":"2023-12-23T10:56:05.002961Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for token in doc1:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.005501Z","iopub.execute_input":"2023-12-23T10:56:05.005851Z","iopub.status.idle":"2023-12-23T10:56:05.012703Z","shell.execute_reply.started":"2023-12-23T10:56:05.005820Z","shell.execute_reply":"2023-12-23T10:56:05.011520Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for token in doc2:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.014132Z","iopub.execute_input":"2023-12-23T10:56:05.014518Z","iopub.status.idle":"2023-12-23T10:56:05.025111Z","shell.execute_reply.started":"2023-12-23T10:56:05.014486Z","shell.execute_reply":"2023-12-23T10:56:05.023920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for token in doc3:\n    print(token)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.026506Z","iopub.execute_input":"2023-12-23T10:56:05.027194Z","iopub.status.idle":"2023-12-23T10:56:05.037026Z","shell.execute_reply.started":"2023-12-23T10:56:05.027148Z","shell.execute_reply":"2023-12-23T10:56:05.035779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**So, it is clearly  seen that Spacy is performing better than the 3 methods stated earlier!**","metadata":{}},{"cell_type":"markdown","source":"# Step 10: Stemming","metadata":{}},{"cell_type":"markdown","source":"Stemming is a text normalization technique used in NLP and information retrieval to reduce words to their base or root form, known as the \"stem.\" The objective of stemming is to group words with similar meanings together by removing prefixes or suffixes, thus treating variations of a word as a common root.\n\nFor example, stemming would transform words like \"running,\" \"runner,\" and \"ran\" to the common stem \"run.\" ","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])\n\nstem_txt1 = \"walking walks walked walk\"\nstem_words(stem_txt1) # Will return to the base word- walk","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.038879Z","iopub.execute_input":"2023-12-23T10:56:05.039517Z","iopub.status.idle":"2023-12-23T10:56:05.052120Z","shell.execute_reply.started":"2023-12-23T10:56:05.039343Z","shell.execute_reply":"2023-12-23T10:56:05.050616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stem_txt2 = \"probably my alltime favorite story is a story of sacrifice and dedication to a noble cause.\"\nstem_words(stem_txt2)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.053961Z","iopub.execute_input":"2023-12-23T10:56:05.054472Z","iopub.status.idle":"2023-12-23T10:56:05.065825Z","shell.execute_reply.started":"2023-12-23T10:56:05.054425Z","shell.execute_reply":"2023-12-23T10:56:05.064487Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The problem of stemming is that sometime stemming gives some root words which doesn't exist(not meaningful) in that language like (probabl,alltim,favorit,stori) etc. In such cases we have to use lemmatization. Lemmatization is a method which works similar as stemming but lemmatization always returns the root words which has meaning.\nLemmatization is slower than stemming because lemmatization again searches for root words with meaning whereas stemming doesn't always care about the meaning of the root word. So if you have to show the output to the user so meaningfulness is important is that case so lemmatization is used here. And if you don't have to show output to the user and you are concerned with the speed then you should use stemming.**","metadata":{}},{"cell_type":"markdown","source":"# Step 11: Lemmatization","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nsentence = \"The cats are running and jumping on the roof.\"\ndoc = nlp(sentence)\nprint(\"{0:15} {1:15}\".format(\"Word\", \"Lemma\"))\nfor token in doc:\n    print(\"{0:15} {1:15}\".format(token.text, token.lemma_))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T10:56:05.067739Z","iopub.execute_input":"2023-12-23T10:56:05.068260Z","iopub.status.idle":"2023-12-23T10:56:06.320861Z","shell.execute_reply.started":"2023-12-23T10:56:05.068216Z","shell.execute_reply":"2023-12-23T10:56:06.319633Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:black; color:white; padding:20px; font-size:24px; font-weight:bold; text-align:left; border-radius:70px;\"> \n      If this notebook helps, please consider UPVOTING.This inspires me a lot 😊 \n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Check out my previous works:\n* [NLP tutorial 1: A detailed introduction to NLP](https://www.kaggle.com/code/faizulislam19095/a-detailed-introduction-to-nlp)\n* [Lung Cancer Prediction- EDA+SMOTE+ML modeling](https://www.kaggle.com/code/faizulislam19095/lung-cancer-prediction-eda-smote-modeling)","metadata":{}}]}